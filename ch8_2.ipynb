{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d86fa1-5648-450f-821a-eb1ab4ede9c7",
   "metadata": {},
   "source": [
    "# DeepDream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed1b7d-34e6-44a0-905a-3691309994cc",
   "metadata": {},
   "source": [
    "DeepDream is an artistic image-modification technique that uses the representations\n",
    "learned by convolutional neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c40885-f889-4dad-b34f-2218a9b58ea4",
   "metadata": {},
   "source": [
    "The DeepDream algorithm is almost identical to the convnet filter-visualization tech-\n",
    "nique introduced in chapter 5, consisting of running a convnet in reverse: doing gra-\n",
    "dient ascent on the input to the convnet in order to maximize the activation of a\n",
    "specific filter in an upper layer of the convnet. DeepDream uses this same idea, with a\n",
    "few simple differences:\n",
    "\n",
    "- With DeepDream, you try to maximize the activation of entire layers rather\n",
    "than that of a specific filter, thus mixing together visualizations of large num-\n",
    "bers of features at once.\n",
    "- You start not from blank, slightly noisy input, but rather from an existing\n",
    "imageâ€”thus the resulting effects latch on to preexisting visual patterns, distort-\n",
    "ing elements of the image in a somewhat artistic fashion.\n",
    "- The input images are processed at different scales (called octaves), which\n",
    "improves the quality of the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d94724-d878-4fc4-9d32-795d532da61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b5c8fe-a2e8-4e29-a0c2-2db380a170ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
      "2022-08-31 16:49:11.405839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-31 16:49:11.407951: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-08-31 16:49:11.877975: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "model = inception_v3.InceptionV3(weights='imagenet',\n",
    "                                 include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fdd13c-5187-4bfe-8975-9389763aaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {\n",
    "    'mixed2': 0.2,\n",
    "    'mixed3': 3.,\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c04d81-bfa9-4542-bb4e-afe9bccefa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "loss = K.variable(0.)\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output\n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss = loss + coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc48bf39-c942-4eb8-9bc2-e382949fd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "dream = model.input\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655a2532-d7be-4035-b144-b2aab45abf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import imageio\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    imageio.imwrite(fname, pil_img)\n",
    "    # scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path)\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "        x /= 2.\n",
    "        x += 0.5\n",
    "        x *= 255.\n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e2227c-7969-4f3b-9bdf-98bfe34a899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image shape (142, 153)\n",
      "Processing image shape (200, 214)\n",
      "Processing image shape (280, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "step = 0.01\n",
    "num_octave = 3\n",
    "octave_scale = 1.4\n",
    "iterations = 20\n",
    "max_loss = 10.\n",
    "base_image_path = 'dogs-vs-cats/train/cats/cat.1.jpg'\n",
    "img = preprocess_image(base_image_path)\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i))\n",
    "        for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "successive_shapes = successive_shapes[::-1]\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)\n",
    "    img = gradient_ascent(img,\n",
    "                          iterations=iterations,\n",
    "                          step=step,\n",
    "                          max_loss=max_loss)\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
    "save_img(img, fname='final_dream.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
