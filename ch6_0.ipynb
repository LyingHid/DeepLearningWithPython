{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf5f6b2-6c05-4c44-9cce-0ce17f7a1092",
   "metadata": {},
   "source": [
    "Chapter 6. Deep learning for text and sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b10d2-6df4-4f4f-9db9-db4623c39bea",
   "metadata": {},
   "source": [
    "The two fundamental deep-learning algorithms for sequence processing are recurrent neural networks and 1D convnets, the one-dimensional version of the 2D convnets that we covered in the previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fe5a6-8da5-41b3-9492-c3d807438f91",
   "metadata": {},
   "source": [
    "“keep in mind throughout this chapter that none of these deep-learning models truly understand text in a human sense;\n",
    "         rather, these models can map the statistical structure of written language, which is sufficient to solve many simple textual\n",
    "         tasks. Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in\n",
    "         much the same way that computer vision is pattern recognition applied to pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b81ae4-8229-4462-934a-adac092f22d4",
   "metadata": {},
   "source": [
    "Like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors. Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways:\n",
    "\n",
    "Segment text into words, and transform each word into a vector.\n",
    "Segment text into characters, and transform each character into a vector.\n",
    "Extract n-grams of words or characters, and transform each n-gram into a vector. N-grams are overlapping groups of multiple consecutive words or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4b962-598d-48e6-8999-29687fab38e8",
   "metadata": {},
   "source": [
    "Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, \n",
    "and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with\n",
    "the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks. There are multiple ways\n",
    "to associate a vector with a token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376bc96c-9d08-4c11-81c6-32dfecd7ce56",
   "metadata": {},
   "source": [
    "two major tokenization:\n",
    "\n",
    "one-hot encoding of tokens\n",
    "token embedding (typically used exclusively for words, and called word embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
